[float]
# Initialized learning rate
lr=1e-3

# Trainning keep probablity for dropout
keep_prob=0.8

[str]
# Running Mode. 
mode=other

# Output dir for trainning checkpoints.
model_path=output/

# Output dir for processed data.
build_path=build/

# Dir for origin corpus or pretrain vectors.
data_path=data/

# Output dir for tensorboard log
log_dir=log/

# Vocab file used
vocab_file=data/vocab.txt

# Pretrained Embedding file
pretrained_file=data/vector.bin

[int]
# Restore network to continue trainning
restore=0

# Epoch to running
epoch=40

# Samples to run per time
batch_size=32

# Peroid to evaluation and save checkpoint
save_period=5

# PAD id
PAD   = 0

# SOS/GO id
SOS   = 1

# EOS/END id
EOS   = 2

# Input sentencse length
T_in  = 30

# Output sentense length
T_out = 30

# Vocab size of input
D_in  = 9132

# Vocab size of onput
D_out = 9132

# Dim of embedding layer in the model
embedding_dim=200

# Dim of hidden layer in the model
hidden_dim=768

# Warmup step for lr
warmup_steps=1000

# Max gradient clip
gradient_clip_num=1

# CNN-para filter num
num_filters=256

[bool]
# Wheather to do word segmentation for corpus
chinese_word_seg = False

# Shuffle the data when spliting the corpus
shuffle = True

# Wheather to use pretrained word vecotr
use_pretrained = False
